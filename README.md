# ğŸŒ¿ GCT Case Study â€“ COâ‚‚ Plant Data Pipeline 

## ğŸ‘‹ Introduction

This project is part of the GCT Case Study Challenge, where Iâ€™ve built a fully functional data pipeline that simulates a COâ‚‚ capturing plant's sensor system. My goal was to ingest real-time data, structure it in the cloud, and automatically process it â€” using only AWS free-tier tools.

This README documents my process, tools, and how the system works end-to-end.

---

## ğŸ§ª Project Summary

- **Sensor Simulation**: 8 virtual sensors generate data every 10 seconds.
- **Data Format**: NDJSON (each line = one reading).
- **Storage**: Every 1 minute, a file containing 6 readings is created.
- **Upload**: Each file is saved to both local disk and S3 using Node-RED.
- **Trigger**: AWS Lambda processes every new file uploaded to S3.
- **Output**: A summary file with stats and alerts is saved to a second S3 bucket.
- **Dashboard**: Data visualized using AWS Glue, Athena, and QuickSight.

---

## ğŸ”§ Tools I Used

| Tool            | Why I Used It                         |
| --------------- | ------------------------------------- |
| **Node-RED**    | To generate and send JSON sensor data |
| **AWS S3**      | For cloud storage (raw + processed)   |
| **AWS Lambda**  | For automatic processing + alerting   |
| **AWS Glue**    | Crawling processed data               |
| **AWS Athena**  | Querying processed data               |
| **SageMaker**   | Forecasting with Prophet              |
| **QuickSight**  | Forecast dashboard (real-time)        |
| **Python 3.12** | Lambda runtime and scripting          |

---

## ğŸ“ Folder Structure in S3

### Input: `co2-plant-raw-data`

```
raw/
  â””â”€â”€ Year YYYY/
      â””â”€â”€ Month MM/
          â””â”€â”€ Day DD/
              â””â”€â”€ HH/
                  â””â”€â”€ sensor_HHMM.json
```

Each JSON file contains 6 lines like this:

```json
{"timestamp":"...","pH":"7.5",...,"co2":"410.0"}
```

---

### Output: `co2-plant-processed-data`

```
processed/
  â””â”€â”€ sensor_HHMM_summary.json
```

Each summary file looks like this:

```json
{
  "source_file": "raw/Year.../sensor_0832.json",
  "summary": {
    "pH": {"mean": 7.2, "std_dev": 0.4},
    "temperature": {"mean": 25.3, "std_dev": 1.8}
  },
  "alert_flags": [
    "âš ï¸ ALERT: pH = 5.8 at 2025-07-12T08:13:40Z"
  ]
}
```

---

## âš™ï¸ Lambda: What It Does

Whenever a file is uploaded to `raw/` in S3:

1. **Reads** the file line by line.
2. **Parses** each reading.
3. **Calculates** mean and standard deviation for each sensor.
4. **Checks** if any value crosses alert thresholds.
5. **Saves** a summary file in `co2-plant-processed-data`.

Thresholds for alerts:

| Sensor        | Rule                  |
| ------------- | --------------------- |
| `pH`          | `< 6.0 or > 8.5`      |
| `temperature` | `< 5Â°C or > 40Â°C`     |
| `pressure`    | `< 1 bar or > 10 bar` |
| `co2`         | `< 300 or > 1000 ppm` |

---

## ğŸ—ƒï¸ S3 Forecasting Folder Structure

### `co2-plant-raw-data`
```
raw/YYYY/MM/DD/HH/sensor_HHMM.json
```

### `co2-plant-processed-data`
```
processed/sensor_HHMM_summary.json
forecasting/
  â”œâ€” input/forecast_data.csv       â† used by SageMaker
  â””â€” output/fill_level_forecast.csv â† generated by Prophet
```

---

## ğŸ”® Forecasting Pipeline (SageMaker + Prophet)

- Created a **Jupyter Notebook** in **SageMaker Studio**
- Used Prophet to train on `fill_level` time series
- Forecasted 24 future hourly values
- Saved forecast CSV back to:
  ```
  s3://co2-plant-processed-data/forecasting/output/fill_level_forecast.csv
  ```

### Sample output:
```csv
ds,yhat
2025-07-15 01:00:00,53.12
2025-07-15 02:00:00,51.87
...
```

---

## ğŸ“Š Real-Time Dashboard (Amazon QuickSight)

- Connected QuickSight to S3 bucket
- Created a dataset from forecast CSV
- Built a **line chart** using:
  - `ds` as time (x-axis)
  - `yhat` as forecast (y-axis)
- Enabled **hourly aggregation** and scheduled dataset refresh
- Updated visuals to avoid default SUM aggregation â€” now using **AVG or raw `yhat`** values per timestamp.
- Set X-axis to **hour-level granularity** for clearer forecasting trends.

### Dashboard Features:
- 24-hour fill level forecast
- Auto-updating via S3 refresh
- Drill-down by hour or custom filters
- Visuals cleaned for precision (fixed SUM/aggregation issues)

---

## âœ… IAM Permissions Note

To enable SageMaker â†’ S3 access:

Attached the following IAM policy to SageMaker execution role:

```json
{
  "Effect": "Allow",
  "Action": ["s3:GetObject", "s3:PutObject", "s3:ListBucket"],
  "Resource": [
    "arn:aws:s3:::co2-plant-processed-data",
    "arn:aws:s3:::co2-plant-processed-data/*"
  ]
}
```

## ğŸ§  What I Learned

- Working with AWS Lambda triggers and permissions (IAM).
- Handling race conditions in S3 file availability.
- Structuring data in the cloud for long-term storage and scalability.
- Debugging tricky JSON issues with newline-delimited formats.
- Using Glue + Athena to power dashboard queries.
- Creating QuickSight visuals directly from Lambda output.

---

## ğŸš§ Future Plans

- Anomaly Detection (ML-Based)
- Time Series Forecasting
- Data Quality Checks
- Cost Optimization & Cold Storage

---

## ğŸ“‚ Repository Structure 

```
ğŸ“ Documents/
  â””â”€â”€ Case Study.pdf                             â† Node-RED flow export
ğŸ“ node-red/
  â””â”€â”€ gct-flow.json                              â† Node-RED flow export
ğŸ“ lambda/
  â””â”€â”€ lambda_function.py                         â† Processing script
ğŸ“ samples/
  â””â”€â”€ sensor_sample.json                         â† Sample input file
ğŸ“ screenshots/
  â””â”€â”€ğŸ“ AWS/                                     â† AWS screenshots
      â””â”€â”€ Dashboard.png                           â† Visual evidence (Node-RED UI, Glue, Athena, QuickSight)
  â””â”€â”€ AWS.png                                     â† Sample Dashboard
ğŸ“„ README.md                                     â† Readme File   
ğŸ“„ Vijay_Presentation.pptx                       â† Case Study Presentation
```
---
## âš™ï¸ Architecture Overview

```mermaid
graph TD
  A[Node-RED Sensor Simulator]
  A --> B[Local Storage - Raw JSON Files]
  A --> C[S3 Bucket - co2-plant-raw-data]
  C --> D[Lambda Trigger - On Upload]
  D --> E[Lambda Function - Aggregation + Alerts]
  E --> F[S3 Bucket - co2-plant-processed-data]
  F --> G1[Forecasting Input CSV for Prophet]
  G1 --> G2[SageMaker Notebook with Prophet]
  G2 --> G3[Forecast Output CSV]
  G3 --> H[AWS Glue Crawler]
  H --> I[Athena SQL Queries]
  G3 --> J[QuickSight Forecast Dashboard]
  F --> H[AWS Glue Crawler]
  H --> I[Athena SQL Queries]
  I --> J[QuickSight Forecast Dashboard]
  I --> J
```
---
## ğŸ¤ About Me

Hi! Iâ€™m Vijay. This case study was a great hands-on experience where I learned to think like a systems integrator â€” working with real-time data, serverless compute, and cloud architecture, all from the ground up.

If youâ€™re reviewing this repo, feel free to reach out. Feedback is welcome!

Mail_id: vijaysdeutsch@gamil.com

